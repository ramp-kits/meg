{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_logo.png\" width=\"250px\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP: predicting the neural origin of magnetic signals\n",
    "<br>\n",
    "<div style=\"text-align: center\">\n",
    "    <em>\n",
    "        <i>Authors: Maria Teleńczuk, Lucy Liu, Hicham Janati, Guillaume Lemaitre, Alexandre Gramfort</i><br>\n",
    "        <a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science</a> (Inria)\n",
    "    </em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "        <td height=\"40%\">\n",
    "            <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/logo_cds.png\" width=\"500px\" ALIGN=”left”/> \n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#Introduction)\n",
    "    - [Origin of electrical signal in the brain](#EEG)\n",
    "    - [Origin of magnetic signal in the brain](#MEG)\n",
    "    - [MEG in practice and problem description](#MEG_in_practice)\n",
    "2. [Data exploration](#Data_exploration)\n",
    "    - [Import Python libraries](#Import)\n",
    "    - [Download the data](#Download_data)\n",
    "    - [MEG recordings](#X) \n",
    "    - [K-nearest neighbors algorithm](#KNN)\n",
    "    - [Use of lead fields](#lars)\n",
    "    - [Lasso Lars algorithm](#lassolars)\n",
    "4. [Performance metric](#Metric)\n",
    "3. [Submission](#Submission) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red\"><b>Note:</b></span> If you are having issues with rendering of this notebook (eg the mathematical formulas are not displaying correctly) please view it on [nbviewer](https://nbviewer.jupyter.org/github/ramp-kits/meg/blob/master/meg_starting_kit.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction <a class=\"anchor\" id=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brain activity produces electrical currents which generate electromagnetic fields. Both electric and magnetic signals resulting from brain activity can be recorded from the scalp of subjects with the help of electroencephalography (EEG) and magnetoencephalography (MEG) respectively.\n",
    "\n",
    "In this challenge, we will focus on the magnetic signals recorded by MEG. Your aim will be to estimate a model that can predict which brain areas are at the origin of the measured MEG signal. This challenge is based on realistic simulations in order to have access to the ground truth and quantitatively evaluate the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin of electrical signals in the brain  <a class=\"anchor\" id=\"EEG\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communication between brain cells (neurons) happens at synapses. That's where the signal is passed from one neuron to another, causing an electrical current to flow within and outside the neuron. A current flowing into one part of the neuron (forming a current sink) flows within the neuron and must leave it elsewhere (forming a current source). The source and sink pair forms a current dipole. The dipole generated by a single neuron can be therefore understood as a vector with constantly changing direction and length.\n",
    "\n",
    "Below you can see a visualization of a positive current (excitatory synaptic input) entering at different locations of 4 neurons. The background color represents the strength of the electric field, the color bar is in nV.\n",
    "Please note, that in real conditions there are many such inputs happening simultaneously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "            <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/4neurons.gif\" width=\"400px\" ALIGN=”left”>\n",
    "        </td>\n",
    "    <td width=\"50%\">\n",
    "        <b>A schema of 4 neurons with the same morphology.</b> Each is stimulated with the synaptic input at different locations (<span style=\"color: #FF0000\">red dot</span>) but at the same time. This is where the current enters the cell. Then, most of it flows through the cell and comes out of the cell body (<span style=\"color: #4B0082\">purple dot</span>) forming a dipole. The direction and relative size of the dipole of each cell is represented by a <span style=\"color: #4B0082\">purple line</span>. Note the differences between the 4 neurons. The colors in the background show the changes in the extracellular electric field.\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Origin of magnetic signals in the brain  <a class=\"anchor\" id=\"MEG\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only talked about the electric currents, but you might remember from your physics class that electrical currents are always associated with a magnetic field. This has been understood since the discovery of Maxwell's equations. Now, if you consider the electrical currents and the dipoles which we discussed above, you can imagine magnetic fields forming closed loops around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "    <td width=\"30%\">\n",
    "         <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/magnetic_schema_small.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    <td width=\"30%\">\n",
    "        The current flow is represented by the purple line (left), and the red lines show the direction of the magnetic field. Those magnetic fields are then recorded by the MEG sensors (grey on the right).\n",
    "    </td>\n",
    "    <td width=\"50%\">\n",
    "        <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_sketch.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurons are constantly active, constantly receiving and propagating electrical signals, but a single neuron is relatively tiny and therefore the potential it generates is too small to be recorded from the scalp. However, there are billions of neurons in the human brain which together form brain structures. Many of the neuron types align and correlate. As you can imagine, in this environment some of the single-cell dipoles are cancelled out while the others add up to form a much stronger signal. This group signal (from the single-cell dipoles that add up) along with a lot of noise can be recorded by MEG or EEG.\n",
    "\n",
    "Keep in mind that due to the alignment of cell bodies in the brain, it is the magnetic fields generated by the intracellular current (i.e. current flowing inside the cell) that generate the signals recorded by MEG, whereas the transmembrane currents (flowing inside/outside the cell) are responsible for EEG signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this is only a simplified explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "     .equalDivide tr td { width:25%; }\n",
    "</style>\n",
    "\n",
    "<table class=\"equalDivide\" cellpadding=\"0\" cellspacing=\"0\" width=\"100%\" border=\"0\">\n",
    "    <tr>\n",
    "    <td width=\"30%\">\n",
    "         <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_device.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    <td width=\"30%\" ALIGN=\"left\">\n",
    "        An actual MEG measurement system (left). It's a VectorView system produced by the <a href=\"https://megin.fi/\">MEGIN company</a>. Such a system contains 306 MEG sensors (right). Each of the 102 blue square contains 3 MEG sensors.\n",
    "    </td>\n",
    "    <td width=\"50%\">\n",
    "        <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/meg_sensors.png\" width=\"250px\" ALIGN=”left” /> \n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEG in practice and problem description<a class=\"anchor\" id=\"MEG_in_practice\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of things happening in the human brain at every moment, so how is it possible to know which signals to look for? The subject participating in a cognitive neuroscience experiment is usually asked to perform the same task multiple times (watching something on a screen, remembering something, pressing buttons etc.). Then the recorded signals obtained during all the repetitions of the experiment are averaged, leading to noise reduction and cleaner data related to that task. This is related to so-called [evoked responses](https://en.wikipedia.org/wiki/Evoked_potential). However, a major challenge facing the neuroscientists is to figure out where exactly the signal is coming from. Solving this problem is known as 'MEG source imaging', 'source localization' or 'electromagnetic brain mapping'. See for example [Baillet et al. 200](https://ieeexplore.ieee.org/document/962275) [Mosher et al. 1999](https://ieeexplore.ieee.org/abstract/document/748978) [Becker et al. (2015)](https://ieeexplore.ieee.org/abstract/document/7298573) or this [article on scholarpedia](http://www.scholarpedia.org/article/Source_localization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mathematical foundations of MEG source imaging\n",
    "\n",
    "If one denotes by $x \\in \\mathbb{R}^p$ the MEG data at one time point, where $p$ is the number of sensors, one has that $x$ is obtained from a linear combination of sources. The linearity is a fact derived from the linearity of Maxwell equations. Assuming the sources can originate from a finite set of $q$ locations in the brain, one has that the data $x$ is related to the source amplitudes $z \\in \\mathbb{R}^q$ via a matrix $L$ of size $p \\times q$, known as the 'lead field matrix', the 'gain' or 'forward operator'. It represents the sensitivity of each sensor to signals from each source location. For example, if the value of $L_{i,j}$ is high, it means that a signal from the $j^{th}$ source with amplitude $z_j$ would result in a high measurement in the $i^{th}$ sensor, i.e. a strong coefficient $x_i$. Conversely, if the value of $L_{i,j}$ is close to 0, it means that a signal from the $j^{th}$ source would result in a very weak measurement in the $i^{th}$ sensor. \n",
    "\n",
    "Assuming the presence of some additive noise $n \\in \\mathbb{R}^p$ it leads to:\n",
    "$$\n",
    "x = Lz + n \\enspace .\n",
    "$$\n",
    "\n",
    "This is a linear regression model, and $z$ corresponds to the regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the source imaging problem, i.e. estimating $z$ from $x$, is the question we ask of you in this challenge: given some simulated MEG data you should predict the brain region(s) (sources) where those signals originate from. We have simulated the MEG data in this challenge as this allows us to know the 'ground truth' source location(s). Each simulation consists of signals generated from one or more sources, which are randomly selected.\n",
    "\n",
    "First, let's explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration <a class=\"anchor\" id=\"Data_exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import <a class=\"anchor\" id=\"Import\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python >= 3.7\n",
    "- [numpy](https://pypi.org/project/numpy/)\n",
    "- [scipy](https://pypi.org/project/scipy/)\n",
    "- [osfclient](https://pypi.org/project/osfclient/)\n",
    "- [pandas](https://pypi.org/project/pandas/)\n",
    "- [pot](https://pypi.org/project/POT/)\n",
    "- [imbalanced-learn](https://pypi.org/project/imbalanced-learn/)\n",
    "- [scikit-learn](https://pypi.org/project/scikit-learn/)\n",
    "- [matplolib](https://pypi.org/project/matplotlib/)\n",
    "- [jupyter](https://pypi.org/project/jupyter/)\n",
    "- [ramp-workflow](https://pypi.org/project/ramp-workflow/)\n",
    "- [ramp-utils](https://github.com/paris-saclay-cds/ramp-board/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will install the required pacakge dependencies, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: ramp-workflow in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (0.4.0.dev0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (0.23.1)\n",
      "Requirement already satisfied: numpy in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (1.18.5)\n",
      "Requirement already satisfied: scipy in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (1.5.0)\n",
      "Requirement already satisfied: joblib in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (0.16.0)\n",
      "Requirement already satisfied: cloudpickle in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (1.4.1)\n",
      "Requirement already satisfied: pandas in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (1.0.1)\n",
      "Requirement already satisfied: click in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-workflow) (7.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from scikit-learn>=0.22->ramp-workflow) (2.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-workflow) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-workflow) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->ramp-workflow) (1.15.0)\n",
      "Requirement already satisfied: ramp-utils in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (0.6.1)\n",
      "Requirement already satisfied: click in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-utils) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-utils) (5.3.1)\n",
      "Requirement already satisfied: pandas in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from ramp-utils) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-utils) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-utils) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from pandas->ramp-utils) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/maja/anaconda3/envs/eeg/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas->ramp-utils) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn\n",
    "!{sys.executable} -m pip install ramp-workflow\n",
    "!{sys.executable} -m pip install ramp-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this notebook running and test your models locally using `ramp-test` (from ramp-workflow), we recommend that you use the Anaconda or Miniconda Python distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data (optional) <a class=\"anchor\" id=\"Download_data\"></a>\n",
    "\n",
    "If the data has not yet been downloaded locally, uncomment the following cell and run it. Be patient. This may take a few minutes. The download size is less than 150 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to find the `test` and `train` folders in the `data/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEG recordings <a class=\"anchor\" id=\"X\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the data we will be using the function called `get_train_data()` and/or 'get_test_data()' defined in `problem.py`. RAMP is also using those functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10',\n",
       "       ...\n",
       "       'e197', 'e198', 'e199', 'e200', 'e201', 'e202', 'e203', 'e204',\n",
       "       'subject', 'L_path'],\n",
       "      dtype='object', length=206)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from problem import get_train_data, get_test_data\n",
    "X_train, y_train = get_train_data()\n",
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has a lot of columns named e1, e2, ... , e204 column named 'subject' and 'L_path'. Each column marked with 'e' is the recording from one of the MEG sensors. We have used 204 sensors for the MEG recordings. It corresponds to the gradiometers in a VectorView system (the other 102 are magnetometers that are ignored here for simplicity). Each row represents the signal from all 204 sensors at one timepoint for one simulation. Recall that each simulation consists of signals generated from one or more sources, which are randomly selected.\n",
    "\n",
    "'subject' is the subject ID of the participant on whom this recording was performed.\n",
    "'L_path' is a path to an additional data for each subject (`lead_field`) which we will discuss later.\n",
    "\n",
    "Let's see what subjects we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['subject_1', 'subject_2', 'subject_3', 'subject_4', 'subject_5'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 206)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of subjects is much smaller than the number of rows because there is a large number of simulations for each subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the heat maps on the head of the first three samples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/topomaps.png\" width=\"500px\" ALIGN=”left” /> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(optional) If you wish to see and run the code that plots the above heatmaps, you will have to additionally install the [MNE-Python](https://pypi.org/project/mne/) library and uncomment the following line and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load plot_topomap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heatmaps are taken from the first three samples of the `train` dataset. The darker the color, the higher is the recorded value.\n",
    "\n",
    "Can you already make a guess how many brain sources lead to the generation of these signals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we look at the ground truth let's discuss what we actually mean by 'source'. The brain is a continuous mass and so we could consider millions of points (individual neurons) to be a potential source. Classically for MEG/EEG source imaging the cortex is discretized. This is illustrated in the following figure where every yellow sphere is the potential location of a source. Here you see about 4000 locations sampled over the left hemisphere. With two hemispheres it leads to about 8000 locations, meaning that one assumes that $q \\approx 8000$. Remember that we have $z \\in \\mathbb{R}^q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/sources_locations.png\" width=\"350px\" ALIGN=”left” />  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While recovering $z$ from $x$ is the ultimate objective of source imaging, $q$ being much larger than $p$ makes this problem really hard. The problem is 'ill-posed'. To simplify the task for the sake of this challenge, we subdivided the brain of each subject into 450 subregions; their technical name is 'parcels' (225 parcels per hemisphere, the <i>corpus callosum</i>, located between the two hemispheres, is excluded). Each parcel is a part of a larger region which has an anatomical meaning (represented by different colors below):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "<img src=\"https://raw.githubusercontent.com/ramp-kits/meg/master/figs/aparc_brain.png\" width=\"500px\" ALIGN=”left” />  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to **predict which parcel(s) the MEG signal originates from**.\n",
    "So let's look at the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you can see is an array mostly filled with 0s. Each column represents one of the 450 brain parcels. A `1`\n",
    " indicates the source of the signal comes from that brain parcel. Each row in the data represents one sample. You can see this problem in machine learning sense as a multioutput binary classification problem (predict if each parcel is active or not) or also as a [multi-label classification problem](https://en.wikipedia.org/wiki/Multi-label_classification) as your task boils down to the prediction of a list of active parcels.\n",
    "\n",
    "You should be able to guess what the shape of the target is, can you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 samples and 450 brain parcels\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {y_train.shape[0]} samples and {y_train.shape[1]} brain parcels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if you guessed correctly the number of sources in each of the three heatmaps above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sources in first three samples: [2. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources_per_sample = np.sum(y_train, axis=1)\n",
    "print(f'Number of sources in first three samples: {n_sources_per_sample[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we simulated this data (using the [MNE-Python](https://mne.tools/stable/index.html) library) we were free to limit the number of sources. Let's check the possible number of sources, in all our samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible number of sources: [1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources_per_sample = np.sum(y_train, axis=1)\n",
    "n_possible_sources = np.unique(n_sources_per_sample)\n",
    "print(f'Possible number of sources: {n_possible_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest neighbors algorithm <a class=\"anchor\" id=\"KNN\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to make some predictions. We will start from the algorithm called k-nearest neighbors. You can read more about it in the [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)  or [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "\n",
    "Let's also look at the 'test' dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 measurements recorded from 5 subjects in the train dataset, and\n",
      "2500 measurements recorded from 5 subjects in the test dataset.\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_test_data()\n",
    "\n",
    "# print info\n",
    "print(f\"There are {len(X_train)} measurements\"\n",
    "      f\" recorded from {len(np.unique(X_train['subject']))} subjects\"\n",
    "      \" in the train dataset, and\\n\"\n",
    "      f\"{len(X_test)} measurements\"\n",
    "      f\" recorded from {len(np.unique(X_test['subject']))} subjects\"\n",
    "      \" in the test dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease calculation time from now on we will only work on the subset of the data but feel free to use the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_data_size(X, y):\n",
    "    # decrease datasize\n",
    "    X = X.reset_index(drop=True)  # make sure the indices are ordered\n",
    "    # 1. take only first 2 subjects\n",
    "    subjects_used = np.unique(X['subject'])[:2]\n",
    "    X = X.loc[X['subject'].isin(subjects_used)]\n",
    "\n",
    "    # 2. take only n_samples from each of the subjects\n",
    "    n_samples = 100  # use only 100 samples/subject\n",
    "    X = X.groupby('subject').apply(\n",
    "        lambda s: s.sample(n_samples, random_state=42))\n",
    "    X = X.reset_index(level=0, drop=True)  # drop subject index\n",
    "\n",
    "    # get y corresponding to chosen X\n",
    "    y = y[X.index]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = decrease_data_size(X_train, y_train)\n",
    "X_test, y_test = decrease_data_size(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all the libraries which we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a `KNeigborsClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Nearest Neihbors\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just use `KNeigborsClassifier` on our data it will not work because the target is multioutput, meaning that we may have more than a single predicted output. That is why we also use [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html), which takes the `KNeighborsClassifier` we just created as an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbors = MultiOutputClassifier(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what we have done so far is not yet sufficient. Because the data X not only consists of the sensor measurements (`dtype`: `float64`) but also of the 'subject' ID and the 'L_path' which are of `dtype`: `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e1         float64\n",
       "e2         float64\n",
       "e3         float64\n",
       "e4         float64\n",
       "e5         float64\n",
       "            ...   \n",
       "e202       float64\n",
       "e203       float64\n",
       "e204       float64\n",
       "subject     object\n",
       "L_path      object\n",
       "Length: 206, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors won't accept this data type. Here, we decide to just drop the whole 'subject' and 'L_path' columns and do not use the information about the subjects. We can do it using [ColumnTranformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) or the function [make_column_transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(('drop', ['subject', 'L_path']),\n",
    "                                       remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a scikit-learn pipeline that first drops the column, and then makes use of our multi-output k-nearest-neighbors classifier.\n",
    "\n",
    "Note: RAMP submissions must consist of a function that returns a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('transformer', preprocessor),\n",
    "        ('classifier', kneighbors)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code presented above is implemented as a sample solution in: `submissions/starting_kit/estimator.py`. If you wish to load it here, uncomment the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/estimator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit this pipeline with the data and make a prediction.\n",
    "\n",
    "For now we will use the Jaccard error \n",
    "(meaning [1-[jaccard score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)]), but please refer to [Performance metric](#Metric) section for the details of the scores based on which you are going to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_knn = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccard error for KNN is 0.995\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "score_knn = 1 - jaccard_score(y_test, y_pred_knn, average='samples')\n",
    "print(f\"The jaccard error for KNN is {score_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted number of sources: [0. 1. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "n_sources_per_sample = np.sum(y_pred_knn, axis = 1)\n",
    "n_sources = np.unique(n_sources_per_sample)\n",
    "print(f'Predicted number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lead fields <a class=\"anchor\" id=\"lars\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at your data files you probably realized that there are more files than just `X.csv` and `target.npz` for 'test' and 'train' in your `data` folder. You also have some files stored in `data/` directory. Their names begin with a subject `id` and end with `\"_L.npz\"` (e.g. `subject_1_L.npz` or `subject_2_L.npz` etc.). As we said earlier in your data X one of the columns called `L_path` corresponds to the paths of those files (note that although we store the path in each row of `X` esentially for each given subject there is a single lead_field file. These files contain the lead field matrices $L \\in \\mathbb{R}^{p \\times q}$ for each of the subjects. Perhaps you have noticed that the `id` corresponds to the `id` of the `subject` in your `X.csv` data file. Let's load one file to see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first path is:  ./data/subject_1_L.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204, 4688)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_subject_1 = X_train[X_train['subject'] == 'subject_1']['L_path'].iloc[0]\n",
    "print('first path is: ', path_subject_1)\n",
    "lead_field = np.load(path_subject_1)\n",
    "L = lead_field['lead_field']\n",
    "L.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each brain is different in shape and structure. Therefore, the signal propagates differently from the source to the sensors in each subject. You have therefore a different lead field for each subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again 204 is the number of MEG sensors and 4688 is the number of possible brain locations (yellow dots above). Note that this is not the number of brain parcels - 450.\n",
    "\n",
    "As we mentioned previously, each brain parcel we consider is a specific 'subregion' in the brain. Each brain parcel is made up of a number of possible locations. The source can be single location within a parcel. The number of locations that make up each parcel varies between subjects (a given parcel can have a different extent for each subject). This information is stored in 'lead_field'. The parcel index that corresponds to any location for a subject is stored in 'lead_field' file.\n",
    "\n",
    "Let's look at the 'lead_field' of another subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 4690)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_subject_2 = X_train[X_train['subject'] == 'subject_2']['L_path'].iloc[0]\n",
    "lead_field = np.load(path_subject_2)\n",
    "lead_field['lead_field'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the number of columns is different from the first 'lead_field' file. This is because, as stated above, the number of locations for each subject can vary a bit.\n",
    "\n",
    "`lead_field['lead_field']` is of shape (n_sensors, n_locations).\n",
    "\n",
    "So how do we match which location belongs to which parcel? In your 'lead_field' file you will find another argument called `parcel_indices`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4690,)\n"
     ]
    }
   ],
   "source": [
    "parcel_indices = lead_field['parcel_indices']\n",
    "print(parcel_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106, 106, 106, 386, 106, 106, 386, 106,  27, 106])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parcel_indices[:10]  # let's look at the index of the first 10 locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4690 values each of which can be one of 450 unique numbers, which correspond to the 450 parcels.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(parcel_indices)} values each of which can be one of \"\n",
    "      f\"{len(np.unique(parcel_indices))} unique numbers, which correspond to the 450 parcels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each number in `parcel_indices` tells us which parcel each point belongs to. The 450 parcel numbers correspond to the parcels represented by the 450 columns in the target `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use this information for the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso algorithm <a class=\"anchor\" id=\"lassolars\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct slightly more complicated estimator which will use 'lead_fields' (later also called '$L$'). First we want to load the 'lead_fields' which are used in our data.\n",
    "\n",
    "*Remark:* Note that we are scaling all the 'lead_fields' by 1e8 in the `get_leadfields` function. That is to avoid having too small numbers given to the estimator.\n",
    "\n",
    "The idea of the Lasso is to estimate $z \\in \\mathbb{R}^q$ as:\n",
    "\n",
    "$\n",
    "\\hat{z} \\in \\mathop{\\mathrm{arg\\,min}}_z \\frac{1}{2p} \\| x - Lz \\|_2^2 + \\alpha \\|z\\|_1\n",
    "$\n",
    "\n",
    "Due to the $\\ell_1$ norm such a strategy leads to a sparse solution, i.e. many entries in $\\hat{z}$ will be 0. We will then look at the non-zero entries in $\\hat{z}$ to know if a parcel is active or not. The higher is $\\alpha$ the less number of active parcels you will estimate. If $\\alpha$ is too high you will then predict that no parcel is active. You can refer to the [wikipedia page on Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics) or this [example on the scikit-learn website](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a class `SparseRegressor` which will accept an estimator (i.e. model) that solves the inverse problem, i.e. estimating $z$ (in the code below called `est_coef`) from $x$ and $L$. Such an estimator is a regressor whose estimated coefficients allow to predict which parcel is active. Note we now use the individual leadfield matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "def _get_coef(est):\n",
    "    \"\"\"Get coefficients from a fitted regression estimator.\"\"\"\n",
    "    if hasattr(est, 'steps'):\n",
    "        return est.steps[-1][1].coef_\n",
    "    return est.coef_\n",
    "\n",
    "\n",
    "class SparseRegressor(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    '''Provided regression estimator (ie model) solves inverse problem\n",
    "        using data X and lead field L. The estimated coefficients (est_coef\n",
    "        sometimes called z) are then used to predict which parcels are active.\n",
    "\n",
    "        X must be of a specific structure with a column name 'subject' and\n",
    "        'L_path' which gives the path to lead_field files for each subject\n",
    "    '''\n",
    "    def __init__(self, model, n_jobs=1):\n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parcel_indices = {}\n",
    "        self.Ls = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) > 0).astype(int)\n",
    "\n",
    "    def _run_model(self, model, L, X):\n",
    "        norms = np.linalg.norm(L, axis=0)\n",
    "        L = L / norms[None, :]\n",
    "\n",
    "        est_coefs = np.empty((X.shape[0], L.shape[1]))\n",
    "        for idx, idx_used in enumerate(X.index.values):\n",
    "            x = X.iloc[idx].values\n",
    "            model.fit(L, x)\n",
    "            est_coef = np.abs(_get_coef(model))\n",
    "            est_coef /= norms\n",
    "            est_coefs[idx] = est_coef\n",
    "\n",
    "        return est_coefs.T\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        for subject_id in np.unique(X['subject']):\n",
    "            if subject_id not in self.Ls:\n",
    "                # load corresponding L if it's not already in\n",
    "                L_used = X[X['subject'] == subject_id]['L_path'].iloc[0]\n",
    "                lead_field = np.load(L_used)\n",
    "                self.parcel_indices[subject_id] = lead_field['parcel_indices']\n",
    "\n",
    "                # scale L to avoid tiny numbers\n",
    "                self.Ls[subject_id] = 1e8 * lead_field['lead_field']\n",
    "                assert (self.parcel_indices[subject_id].shape[0] ==\n",
    "                        self.Ls[subject_id].shape[1])\n",
    "\n",
    "        n_parcels = np.max([np.max(s) for s in self.parcel_indices.values()])\n",
    "        betas = np.empty((len(X), n_parcels))\n",
    "        for subj_idx in np.unique(X['subject']):\n",
    "            L_used = self.Ls[subj_idx]\n",
    "\n",
    "            X_used = X[X['subject'] == subj_idx]\n",
    "            X_used = X_used.drop(['subject', 'L_path'], axis=1)\n",
    "\n",
    "            est_coef = self._run_model(self.model, L_used, X_used)\n",
    "\n",
    "            beta = pd.DataFrame(\n",
    "                np.abs(est_coef)\n",
    "            ).groupby(self.parcel_indices[subj_idx]).max().transpose()\n",
    "            betas[X['subject'] == subj_idx] = np.array(beta)\n",
    "        return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model_lars = linear_model.LassoLars(alpha=1.0, max_iter=3,\n",
    "                                    normalize=False,\n",
    "                                    fit_intercept=False)\n",
    "\n",
    "lasso_lars = SparseRegressor(model_lars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_lars.fit(X_train, y_train)\n",
    "y_pred_lassolars = lasso_lars.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard error for the Lasso Lars algorithm using lead fields is 1.0\n"
     ]
    }
   ],
   "source": [
    "score_lassolars = 1 - jaccard_score(y_test, y_pred_lassolars, average='samples')\n",
    "print(f'Jaccard error for the Lasso Lars algorithm using lead fields is {score_lassolars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is very bad. Let's see how well the number of sources is predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted possible number of sources: [0]\n"
     ]
    }
   ],
   "source": [
    "n_sources_by_sample = np.sum(y_pred_lassolars, axis = 1)\n",
    "n_sources = np.unique(n_sources_by_sample)\n",
    "print(f'Predicted possible number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in fact, the `LassoLars` with these settings predicted no sources at all for every sample.\n",
    "\n",
    "Let's try to improve our algorithm. What could we change?\n",
    "\n",
    "\n",
    "As we mentioned previously when $\\alpha$ is set too high the algorithm will predict no active parcels. We previously set `alpha` to 1.0. We will now set it taking into account the data. We will define a custom scikit-learn estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import RegressorMixin\n",
    "\n",
    "class CustomSparseEstimator(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" Regression estimator which uses LassoLars algorithm with given alpha\n",
    "        normalized for each lead field L and x. \"\"\"\n",
    "    def __init__(self, alpha=0.2):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, L, x):\n",
    "        alpha_max = abs(L.T.dot(x)).max() / len(L)\n",
    "        alpha = self.alpha * alpha_max\n",
    "        lasso = linear_model.LassoLars(alpha=alpha, max_iter=3,\n",
    "                                       normalize=False, fit_intercept=False)\n",
    "        lasso.fit(L, x)\n",
    "        self.coef_ = lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomSparseEstimator(alpha=0.2)\n",
    "lasso_lars_alpha = SparseRegressor(custom_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_lars_alpha.fit(X_train, y_train)\n",
    "y_pred_alpha = lasso_lars_alpha.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard error for the Lasso Lars algorithm using lead fields with updating alpha is 0.7268333333333333\n"
     ]
    }
   ],
   "source": [
    "score_alpha = 1 - jaccard_score(y_test, y_pred_alpha, average='samples')\n",
    "print(f'Jaccard error for the Lasso Lars algorithm using lead fields with updating alpha is {score_alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible number of sources: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "n_sources_by_sample = np.sum(y_pred_alpha, axis=1)\n",
    "n_sources = np.unique(n_sources_by_sample)\n",
    "print(f'Possible number of sources: {n_sources}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the above algorithm in RAMP you need to change it to be a function named `get_estimator` that returns a scikit-learn type of pipeline. This is saved in the `submissions/lasso_lars/estimator.py`. You can load the code here by uncommenting the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'submissions/lasso_lars/estimator.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our estimator is predicting a more feasible number of sources at each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric <a class=\"anchor\" id=\"Metric\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your algorithm will be evaluated based on two scores:\n",
    "- [Jaccard error](#jaccard), and\n",
    "- [EMD (Earth Mover's Distance) score](#emd)\n",
    "\n",
    "You want to aim for the best score you can achieve in both metric which is 0, while you want to avoid the worst score of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard error <a class=\"anchor\" id=\"jaccard\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard score is giving the higher score to the better results, that's why we subtract it from 1 to get a score which we now call Jaccard error. \n",
    "\n",
    "In simple terms Jaccard score is the size of the intersection of the true and predicted parcels divided by the size of the union:\n",
    "\n",
    "$$\n",
    "J_{accard\\_score}(y_{true},y_{pred}) = \\frac{|y_{true} \\cap y_{pred}|}{|y_{true} \\cup y_{pred}|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J_{accard\\_error} = 1-J_{accard\\_score}\n",
    "$$\n",
    "\n",
    "where $y_{true}$ and $y_{pred}$ are the sets of true and predicted parcels.\n",
    "\n",
    "\n",
    "You can read more on the Jaccard score in [Scikit-learn definition](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) (which is the implementation we are using here).\n",
    "\n",
    "We get the following Jaccard error for the algorithms we discussed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jaccard error for KNN model is 0.995,\n",
      "for the model which predicts only 0s is 1.0,\n",
      "for SparseRegressor with LassoLars as a model and updating alpha is 0.7268333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "score_knn = 1 - jaccard_score(y_test, y_pred_knn, average='samples')\n",
    "score_lassolars = 1 - jaccard_score(y_test, y_pred_lassolars, average='samples')\n",
    "score_alpha = 1 - jaccard_score(y_test, y_pred_alpha, average='samples')\n",
    "\n",
    "print(f'The Jaccard error for KNN model is {score_knn},')\n",
    "print(f'for the model which predicts only 0s is {score_lassolars},')\n",
    "print(f'for SparseRegressor with LassoLars as a model and updating alpha is {score_alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth Mover's Distance (EMD) score <a class=\"anchor\" id=\"emd\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMD is using precalculated minimum distances between each of the 450 brain parcels to know the distance between the true ($y_{true}$) and the predicted ($y_{pred}$) parcels when calculating the score.\n",
    "\n",
    "The formula for EMD is: \n",
    "\n",
    "$$\n",
    "EMD(y_{true}, y_{pred}) = \\frac{\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{n}f_{i,j}d_{i,j}}{\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{n}f_{i,j}}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of true active parcels and $n$ is the number of predicted active parcels. $d_{i,j}$ is the distance between $y_{true,i}$ and $y_{pred,j}$ parcels. $f_{i,j}$ is the solution to the optimization problem of transforming predicted parcels ($y_{pred,j}$) to true parcels ($y_{true,i}$).\n",
    "\n",
    "You can read more about Earth Mover’s Distance (EMD) in [Wikipedia](https://en.wikipedia.org/wiki/Earth_mover%27s_distance#:~:text=In%20statistics%2C%20the%20earth%20mover's,known%20as%20the%20Wasserstein%20metric.). We are using [Pot](https://pythonot.github.io/index.html) Python libarary to perform the calculations, if you are interested in the implementation details you can also look into [this](https://pythonot.github.io/gen_modules/ot.lp.html#ot.lp.emd2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMD score will give us the following results for the algorithms described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The EMD score for KNN model is 0.9253375070940231,\n",
      "for the model which predicts only 0s is 1.0,\n",
      "for SparseRegressor with LassoLars as a model and updating alpha is 0.31736206165668507\n"
     ]
    }
   ],
   "source": [
    "from problem import EMDScore\n",
    "emdscore = EMDScore()\n",
    "emd_knn = emdscore(y_test, y_pred_knn)\n",
    "emd_lassolars = emdscore(y_test, y_pred_lassolars)\n",
    "emd_alpha = emdscore(y_test, y_pred_alpha)\n",
    "\n",
    "print(f'The EMD score for KNN model is {emd_knn},')\n",
    "print(f'for the model which predicts only 0s is {emd_lassolars},')\n",
    "print(f'for SparseRegressor with LassoLars as a model and updating alpha is {emd_alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a class=\"anchor\" id=\"Submission\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost ready to begin, however before you do we want to make the last point about the data and on how RAMP works with it behind the scenes. \n",
    "\n",
    "You received quite large dataset with 10K samples per subject to train your models and 500 samples per subject to test it with 5 different subjects in the train and 5 different subjects in the test dataset. \n",
    "\n",
    "When you submit your model to RAMP, your model will be trained and tested on a different datasets with the same characteristics: 10K samples/subject and 500 samples/subject in the train and test datasets respectively (with 10 new subjects). You will be judged on the score calculated on the predictions made on these datasets.\n",
    "\n",
    "You are always allowed to make only a single submission at the time (you can make next submission after your code finishes training or if it fails).\n",
    "\n",
    "Therefore think carefully if you wish to use the whole dataset available to you. It's up to you which part of the dataset you will use: to limit the dataset you will need to add `resampler` to your pipeline. This might be done as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, instead of Sklearn Pipeline we are going to use Pipeline from [imbalanced-learn](https://imbalanced-learn.org/stable/auto_examples/pipeline/plot_pipeline_classification.html#sphx-glr-auto-examples-pipeline-plot-pipeline-classification-py). We cannot use Sklearn Pipeline because as for now it does not allow for the change of a target `y`.\n",
    "\n",
    "As an example we will use as above a `k-nearest neighbors` classifier and add a resampling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "    \n",
    "class Resampler(BaseEstimator):\n",
    "    ''' Resamples X and y to every 100th sample '''\n",
    "    def fit_resample(self, X, y):\n",
    "        return X[::100], y[::100]  # take 1% of data\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "kneighbors = MultiOutputClassifier(clf)\n",
    "preprocessor = make_column_transformer(('drop', ['subject', 'L_path']),\n",
    "                                       remainder='passthrough')\n",
    "res = Resampler()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('downsampler', res),\n",
    "    ('transformer', preprocessor),\n",
    "    ('classifier', kneighbors)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start from the full training set and reduce it using\n",
    "# resampler\n",
    "X_train, y_train = get_train_data()\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score for downsampled KNN model is \n",
      "EMD: 0.788223595972351,\n",
      "jaccard_score: 0.9786666666666667\n"
     ]
    }
   ],
   "source": [
    "emd_dknn = emdscore(y_test, y_pred)\n",
    "score_dknn = 1 - jaccard_score(y_test, y_pred, average='samples')\n",
    "print(f'The score for downsampled KNN model is \\nEMD: {emd_dknn},\\njaccard_score: {score_dknn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included this solution as a third example solutions for RAMP under the name: `knn_resample`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your submission locally first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you found a good model you wish to test you should place it in a directory, naming it as you wish, and place it in the `submissions/` folder (you can already find there two submissions in the folders `submissions/starting_kit` and `submissions/lasso_lars` which we talked about above). The file placed in your submission directory (e.g., `starting_kit/` should be called `estimator.py` and should define a function called `get_estimator` that returns a scikit-learn type of pipeline.\n",
    "\n",
    "You can then test your submission locally using the command:\n",
    "\n",
    "`ramp-test --submission <your submission folder name>`\n",
    "\n",
    "if you prefer to run a quick test on much smaller subset of data you can add `--quick-test` option:\n",
    "\n",
    "`ramp-test --submission <your submission folder name> --quick-test`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you wish to run our lasso_lars with alpha alrgorithm as a quick-test you can do so as follows \n",
    "\n",
    "<i> Remark:</i> `!` is used to tell jupyter notebook to execute a shell command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178m\u001b[1mTesting Source localization of MEG signal\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading train and test files from ./data ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading cv ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mTraining submissions/lasso_lars ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 0\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    EMD  jaccard error      time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.324\u001b[0m          \u001b[38;5;150m0.707\u001b[0m  \u001b[38;5;150m0.012347\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.326\u001b[0m          \u001b[38;5;105m0.714\u001b[0m  \u001b[38;5;105m3.635020\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.326\u001b[0m          \u001b[38;5;218m0.726\u001b[0m  \u001b[38;5;218m3.435726\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 1\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    EMD  jaccard error      time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.326\u001b[0m          \u001b[38;5;150m0.714\u001b[0m  \u001b[38;5;150m0.000871\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.324\u001b[0m          \u001b[38;5;105m0.707\u001b[0m  \u001b[38;5;105m3.760737\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.326\u001b[0m          \u001b[38;5;218m0.726\u001b[0m  \u001b[38;5;218m4.069131\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mMean CV scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore             EMD   jaccard error        time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.325\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m\u001b[38;5;150m0.0\u001b[0m009\u001b[0m  \u001b[38;5;150m0.711\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m\u001b[38;5;150m0.0\u001b[0m035\u001b[0m  \u001b[38;5;150m0.0\u001b[0m \u001b[38;5;150m\u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;150m0.0\u001b[0m1\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.325\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.0009\u001b[0m  \u001b[38;5;105m0.711\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.0035\u001b[0m  \u001b[38;5;105m3.7\u001b[0m \u001b[38;5;105m\u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;105m0.06\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m      \u001b[38;5;1m\u001b[1m\u001b[38;5;218m0.32\u001b[0m6\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m\u001b[38;5;218m0.0\u001b[0m\u001b[0m     \u001b[38;5;218m0.726\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m\u001b[38;5;218m0.0\u001b[0m\u001b[0m  \u001b[38;5;218m3.8\u001b[0m \u001b[38;5;218m\u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m\u001b[0m \u001b[38;5;218m0.32\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mBagged scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore    EMD  jaccard error\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.325\u001b[0m          \u001b[38;5;105m0.711\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.326\u001b[0m          \u001b[38;5;218m0.726\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! ramp-test --submission lasso_lars --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the score may vary from what we have seen before because we are using a different selection of the data.\n",
    "\n",
    "For more information on how to submit your code on [ramp.studio](https://ramp.studio/), refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
